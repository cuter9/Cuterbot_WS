{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road Follower - Train Model by using host PC\n",
    "\n",
    "In this notebook we will train a neural network to take an input image, and output a set of x, y values corresponding to a target.\n",
    "\n",
    "We will be using PyTorch deep learning framework to train pytorch model, e.g. ResNet18, neural network architecture model for road follower application.\n",
    "\n",
    "Before executing this script for training pytroch models, the system should install python3 and some packages as followings:\n",
    "1. Install version 3.6 or above (e.g. python3.8), and set the installed python as the Pycharm project interpreter.\n",
    "     \n",
    "2. Install pytorch and torchvision packages with Nvidia cuda capability in host PC by executing the command in Windows PowerShell command window as following (the pytorch version 2.4.0+cu121 and torchvision version 0.19.0+cu121 is workable in python 3.8), \n",
    ">       pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "3. Install python packages jupyter and jupyterlab:\n",
    ">       pip install jupyter jupyterlab\n",
    "\n",
    "4. The you can run command \"jupyter lab\" in command window to start training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import glob\n",
    "import PIL.Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "### The following pytorch models may be used for images classification. \n",
    "### The \"mobilenet_X\" models would have supporting issues when converting them to TensorRT engine using torch2trt.\n",
    "### Thus using mobilenet would be only applicable for pytorch simulation but not for TensorRT.\n",
    "\n",
    "# vgg11\n",
    "# googlenet\n",
    "# inception_v3\n",
    "# resnet18, resnet34, resnet50, resnet101; 18, 34, 50, ... no of layers of network\n",
    "# densenet121, densenet161, densenet169, densenet201; 121, 161, ... no of layers of network\n",
    "# shufflenet_v2_x2_0, shufflenet_v2_x1_5, shufflenet_v2_x1_0, shufflenet_v2_x0_5 xy_y: factor determines depth of tensor (no. of channels or no. of filters of each stage)\n",
    "# mnasnet1_3, mnasnet1_0, mnasnet0_75, mnasnet0_5; y_y: MnasNet_A1 model, depth multiplier determines depth of tensor (no. of channel, no. of filters)\n",
    "# mobilenet, mobilenet_v2, mobilenet_v3_large, mobilenet_v3_small\n",
    "# efficientnet_b0~b5, bx: using different compound coefficien phi\n",
    "\n",
    "TRAIN_MODEL = 'densenet161'\n",
    "\n",
    "# *** refererence : https://pytorch.org/docs/stable/optim.html#algorithms\n",
    "# use the following learning algorithms for evaluation\n",
    "# \"Adam\", \"SGD\", \"ASGD\", \"Adadelta\", \"RAdam\"; the parameters lr=0.01, momentum=0.92 is needed for SGD\n",
    "TRAIN_METHOD = 'Adam'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and extract data\n",
    "\n",
    "1. Before you start, you should upload the zip file that you created in the ``data_collection.ipynb`` notebook on the robot. \n",
    "\n",
    "> If you're training on the JetBot you collected data on, you can skip this!\n",
    "\n",
    "2. After training is finished, the trained model will be store in the diretory set by dir_depo.\n",
    "The default dir_depo is 'D:\\\\AI_Lecture_Demos\\\\Data_Repo\\\\Cuterbot_Repo', thus if you want to store in another directory, tou may set this variable in the following cell. \n",
    "3. The data set file name (variable name: training_datafile) should set to the exact same file name used in ``data_collection.ipynb``, which would have format \"road_following_{DATASET_DIR}_{timestr()}.zip\"; but, the example of variable training_datafile used here is 'dataset_xy_0916_1.zip'.\n",
    "4. You should then extract the dataset by calling the command below:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# !unzip -q road_following.zip\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# The dir_depo parameter can be set as you required: \n",
    "dir_depo = 'D:\\\\AI_Lecture_Demos\\\\Data_Repo\\\\Cuterbot_Repo'\n",
    "# dir_depo = 'D:\\\\AI_Lecture_Demos\\\\Data_Repo\\\\Cuterbot_2004_Repo'\n",
    "os.makedirs(dir_depo, exist_ok=True)\n",
    "# dir_depo = os.getcwd()\n",
    "training_datafile = 'dataset_xy_0916_1.zip'  # check the data file is loaded to dir_depo\n",
    "\n",
    "with ZipFile(os.path.join(dir_depo, training_datafile), 'r') as zObject:\n",
    "    zObject.extractall(path=dir_depo)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a folder named ``dataset_xy`` appear in the file directory as set in variable \"dir_depo\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset Instance\n",
    "\n",
    "Here we create a custom ``torch.utils.data.Dataset`` implementation, which implements the ``__len__`` and ``__getitem__`` functions.  This class\n",
    "is responsible for loading images and parsing the x, y values from the image filenames.  Because we implement the ``torch.utils.data.Dataset`` class,\n",
    "we can use all of the torch data utilities :)\n",
    "\n",
    "We hard coded some transformations (like color jitter) into our dataset.  We made random horizontal flips optional (in case you want to follow a non-symmetric path, like a road\n",
    "where we need to 'stay right').  If it doesn't matter whether your robot follows some convention, you could enable flips to augment the dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_x(path, width):\n",
    "    \"\"\"Gets the x value from the image filename\"\"\"\n",
    "    return (float(int(path.split(\"_\")[1])) - width/2) / (width/2)\n",
    "\n",
    "def get_y(path, height):\n",
    "    \"\"\"Gets the y value from the image filename\"\"\"\n",
    "    return (float(int(path.split(\"_\")[2])) - height/2) / (height/2)\n",
    "\n",
    "class XYDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, directory, random_hflips=False):\n",
    "        self.directory = directory\n",
    "        self.random_hflips = random_hflips\n",
    "        self.image_paths = glob.glob(os.path.join(self.directory, '*.jpg'))\n",
    "        self.color_jitter = transforms.ColorJitter(0.3, 0.3, 0.3, 0.3)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        \n",
    "        image = PIL.Image.open(image_path)\n",
    "        width, height = image.size\n",
    "        x = float(get_x(os.path.basename(image_path), width))\n",
    "        y = float(get_y(os.path.basename(image_path), height))\n",
    "      \n",
    "        if float(np.random.rand(1)) > 0.5:\n",
    "            image = transforms.functional.hflip(image)\n",
    "            x = -x\n",
    "        \n",
    "        image = self.color_jitter(image)\n",
    "        if TRAIN_MODEL == 'inception_v3':\n",
    "            image = transforms.functional.resize(image, (299, 299))\n",
    "        else:\n",
    "            image = transforms.functional.resize(image, (224, 224))\n",
    "        image = transforms.functional.to_tensor(image)\n",
    "        image = image.numpy()[::-1].copy()\n",
    "        image = torch.from_numpy(image)\n",
    "        image = transforms.functional.normalize(image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        \n",
    "        return image, torch.tensor([x, y]).float()\n",
    "    \n",
    "dataset = XYDataset(os.path.join(dir_depo, 'dataset_xy'), random_hflips=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train and test sets\n",
    "Once we read dataset, we will split data set in train and test sets. In this example we split train and test a 90%-10%. The test set will be used to verify the accuracy of the model we train."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test_percent = 0.1\n",
    "num_test = int(test_percent * len(dataset))\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - num_test, num_test])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data loaders to load data in batches\n",
    "\n",
    "We use ``DataLoader`` class to load data in batches, shuffle data and allow using multi-subprocesses. In this example we use batch size of 64. Batch size will be based on memory available with your GPU and it can impact accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Neural Network Model \n",
    "\n",
    "1. In a process called $transfer$  $learning$, we can repurpose a pre-trained model (trained on millions of images) for a new task that has possibly much less data available.\n",
    "More Details on Transfer Learning: https://www.youtube.com/watch?v=yofjFQddwHE \n",
    "\n",
    "2. The $transfer$  $learning$ will be used for training the model for road following  simulation.\n",
    "\n",
    "3. You can use the models (the parameter setting TRAIN_MODEL above) available in PyTorch TorchVision package, such as resnet18, resnet34, resnet50, resnet101, mobilenet_v2, vgg11, mobilenet_v3_large, inception_v3, efficientnet_b4, googlenet.\n",
    "\n",
    "4. Before you use the pre-trained pytorch model for transfer learning, you should modify the classifier nodes parameters of the neural model architecture. \n",
    "\n",
    "5. The modification should be done through modifying the function \"load_tune_pth_model\" in \"model_selection.py\" which is located in the director jetbot/utils/. \n",
    "> e.g. ResNet model has fully connect (fc) final layer with 512 as in_features and we will be training for regression thus out_features as 2\n",
    "\n",
    "6. Please refer the information in the following web sites for modifying pytorch pre-trained models:\n",
    "* classifier : https://pytorch.org/vision/0.10/models.html#classification\n",
    "* github : https://github.com/pytorch/vision/tree/release/0.11/torchvision/models\n",
    "* More details on ResNet-18 and other variants: https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "\n",
    "7. After modifying the classifier, load the model below:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "%cd \"../../jetbot/utils\"\n",
    "from model_selection import load_tune_pth_model\n",
    "model, model_type = load_tune_pth_model(pth_model_name=TRAIN_MODEL, pretrained=True)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processor used for training\n",
    "1. You may use CPU or GPU for training the model by checking the check widget below.\n",
    "2. If you use GPU for training, the model you selected (in \"TRAIN_MODEL\") will be transferred for execution on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ** you may use CPU or GPU for training\n",
    "processor = 'GPU'\n",
    "if processor == 'GPU':\n",
    "    device = torch.device('cuda')\n",
    "    print(\"torch cuda version : \", torch.version.cuda)\n",
    "    print(\"cuda is available for pytorch: \", torch.cuda.is_available())    \n",
    "elif processor == 'CPU':\n",
    "    device = torch.device('cpu')\n",
    "model = model.float()\n",
    "model = model.to(device, dtype=torch.float)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Regression:\n",
    "\n",
    "1. The training record is stored in directory set by vaieable \"dir_training_records\" in the cell below.\n",
    "2. The trained model for road following is saved in directory \"DIR_RC_MODEL_REPO\" with file name \"BEST_MODEL_PATH\" in the cell below, and the 'torch_model_tbl.csv' in the directory \"DIR_MODEL_REPO\" will be updated accordingly. \n",
    "3. We train for 70 epochs and save best model if the loss is reduced.\n",
    "4. The parameters or variables you may be modified as you want.\n",
    "\n",
    "### Define Neural Network Model Learning Algorithm:\n",
    "1. You can use the following learning algorithms (the parameter setting TRAIN_METHOD above) for training a model\n",
    "\"Adam\", \"SGD\", \"ASGD\", \"Adadelta\", \"RAdam\"; the parameters lr=0.01, momentum=0.92 is needed for SGD.\n",
    "* Reference web site : https://pytorch.org/docs/stable/optim.html#algorithms"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%cd \"../../jetbot/utils\"\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from training_profile import * \n",
    "\n",
    "dir_training_records = os.path.join(dir_depo, processor, 'training records', \"road_following\", TRAIN_MODEL)\n",
    "os.makedirs(dir_training_records, exist_ok=True)\n",
    "\n",
    "DIR_MODEL_REPO = os.path.join(dir_depo, processor, 'model_repo')\n",
    "os.makedirs(DIR_MODEL_REPO, exist_ok=True)\n",
    "DIR_RC_MODEL_REPO = os.path.join(DIR_MODEL_REPO, 'road_following')\n",
    "os.makedirs(DIR_RC_MODEL_REPO, exist_ok=True)\n",
    "# BEST_MODEL_PATH = 'best_steering_model_xy.pth'\n",
    "BEST_MODEL_PATH = os.path.join(DIR_RC_MODEL_REPO, \"best_steering_model_xy_\" + TRAIN_MODEL + \".pth\")\n",
    "\n",
    "NUM_EPOCHS = 70\n",
    "best_loss = 1e9\n",
    "\n",
    "optimizer = getattr(optim, TRAIN_METHOD)(model.parameters(), weight_decay=0)\n",
    "# optimizer = getattr(optim, TRAIN_METHOD)(model.parameters(), lr=0.01, momentum=0.95)\n",
    "\n",
    "loss_data = []\n",
    "lt_epoch = []  # learning time per epoch\n",
    "lt_sample = []  # learning time per epoch\n",
    "\n",
    "print(\"start training model ----- %s -----\" % TRAIN_MODEL)\n",
    "\n",
    "batch_size = len(train_loader)\n",
    "pbar_overall_format = \"{desc} {percentage:.2f}% | {bar} | elapsed: {elapsed}; estimated to finish: {remaining}\"\n",
    "pbar_overall = tqdm(total=100, bar_format = pbar_overall_format)\n",
    "show_batch_progress = False   # Set True if need to show the batch learning progress in an epoch\n",
    "show_training_plot = False  # Set True if need to show the converbent profile during training\n",
    "\n",
    "best_loss = None\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_epoch = time.time()\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    if show_batch_progress:\n",
    "        pbar_batch = tqdm(train_loader, total = batch_size)\n",
    "    else:\n",
    "        pbar_batch = iter(train_loader)\n",
    "    for index, (images, labels) in enumerate(pbar_batch):\n",
    "        start_sample = time.time()\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        if TRAIN_MODEL == 'inception_v3':\n",
    "            outputs = model(images)\n",
    "            loss_main = F.mse_loss(outputs.logits, labels, reduction='mean')\n",
    "            loss_aux = F.mse_loss(outputs.aux_logits, labels, reduction='mean')\n",
    "            loss = loss_main + 0.3 * loss_aux\n",
    "            \n",
    "        elif TRAIN_MODEL == 'googlenet':\n",
    "            outputs = model(images)\n",
    "            loss_main = F.mse_loss(outputs.logits, labels, reduction='mean')\n",
    "            loss_aux1 = F.mse_loss(outputs.aux_logits1, labels, reduction='mean')\n",
    "            loss_aux2 = F.mse_loss(outputs.aux_logits2, labels, reduction='mean')\n",
    "            loss = loss_main + 0.3 * loss_aux1 + 0.3 * loss_aux2\n",
    "            \n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = F.mse_loss(outputs, labels, reduction='mean')\n",
    "            \n",
    "        train_loss += float(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        end_sample = time.time()\n",
    "        lt_sample.append(end_sample - start_sample)\n",
    "        \n",
    "        pbar_overall.update(round(100/(NUM_EPOCHS*batch_size), 2))\n",
    "        pbar_overall.set_description(desc = f'Overall progress - Epoch [{epoch+1}/{NUM_EPOCHS}]')\n",
    "        pbar_overall.set_postfix(best_loss = best_loss, train_loss = train_loss/(index+1))\n",
    "               \n",
    "        if show_batch_progress:\n",
    "            pbar_batch.set_description(desc = f'Progress in the epoch {epoch+1} ')\n",
    "            pbar_batch.set_postfix(mean_batch_loss = train_loss/(index+1))\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    for images, labels in iter(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = F.mse_loss(outputs, labels)\n",
    "        test_loss += float(loss)\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    end_epoch = time.time()\n",
    "    lt_epoch.append(end_epoch - start_epoch)\n",
    "      \n",
    "    if best_loss == None or test_loss < best_loss :\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "        best_loss = test_loss\n",
    "    \n",
    "    loss_data.append([train_loss, test_loss])\n",
    "    \n",
    "# function plot_loss(loss_data, best_loss, no_epoch, dir_training_records, train_model, train_method) is in jetbot.utils\n",
    "    if epoch == NUM_EPOCHS:\n",
    "        show_training_plot=True\n",
    "    plot_loss(loss_data=loss_data, best_loss=best_loss, no_epoch=NUM_EPOCHS,\n",
    "              dir_training_records=dir_training_records, # the directory stored training records\n",
    "              train_model=TRAIN_MODEL, train_method=TRAIN_METHOD, processor=processor, # this 2 parameters are for plot title only\n",
    "              show_training_plot=show_training_plot)\n",
    "\n",
    "overall_time = pbar_overall.format_dict[\"elapsed\"]\n",
    "# function lt_plot(lt_epoch, lt_sample, dir_training_records, train_model, train_method) is in jetbot.utils\n",
    "lt_plot(lt_epoch=lt_epoch, lt_sample=lt_sample, overall_time=overall_time,\n",
    "        dir_training_records=dir_training_records, # the directory stored training records\n",
    "        train_model=TRAIN_MODEL, train_method=TRAIN_METHOD, processor=processor) # this 2 parameters are for plot title only"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_file = os.path.join(DIR_MODEL_REPO, 'torch_model_tbl.csv')\n",
    "model_path = \"./road_following/best_steering_model_xy_\" + TRAIN_MODEL + \".pth\"\n",
    "\n",
    "if os.path.isfile(df_file):\n",
    "    df = pd.read_csv(df_file, header=None)\n",
    "else:\n",
    "    df = pd.DataFrame()\n",
    "df = df._append([[\"classifier\", model_type, model_path]], ignore_index = False)\n",
    "df = df.drop_duplicates()\n",
    "df.to_csv(df_file, header=False, index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print('Training is completed! \\n you can close the figures by restart the kernel!')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, it will generate ``best_steering_model_xy_<TRAIN_MODEL>.pth``file in the directory as set in \"DIR_RC_MODEL_REPO\", which you can use for inferencing in the live demo notebook.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
