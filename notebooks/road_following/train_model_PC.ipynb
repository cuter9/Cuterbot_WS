{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Road Follower - Train Model by using host PC\n",
    "\n",
    "In this notebook we will train a neural network to take an input image, and output a set of x, y values corresponding to a target.\n",
    "\n",
    "We will be using PyTorch deep learning framework to train pytorch model, e.g. ResNet18, neural network architecture model for road follower application.\n",
    "\n",
    "Before executing this script for training pytroch models, the system should install python3 and some packages as followings:\n",
    "1. Install version 3.6 or above (e.g., python3.8), and set the installed python as the Pycharm project interpreter.\n",
    "     \n",
    "2. Make sure Nvidia CUDA has been installed in your PC. To install CUDA, you may select the CUDA version and follow the instructions with reference to the Nvidia Website below. https://developer.nvidia.com/cuda-toolkit-archive\n",
    "\n",
    "3. Install pytorch and torchvision packages with an Nvidia CUDA version (e.g., version 12.4.1) in host PC by executing the command in a Windows PowerShell command window as following (the pytorch version 2.4.1+cu124 and torchvision version 0.19.1+cu124 is workable in python 3.8). You may refer to: https://pytorch.org/get-started/previous-versions/\n",
    ">       pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "4. Install python packages jupyter and jupyterlab:\n",
    ">       pip install jupyter jupyterlab\n",
    "\n",
    "5. Then you can run command \"jupyter lab\" in a command window to start the training.\n",
    "6. During training, you may get `Import Error` of some python packages need for executing the installation, you have to install the python packages as required."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import glob\n",
    "import PIL.Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "### The following pytorch models may be used for images classification. \n",
    "### The \"mobilenet_X\" models would have supporting issues when converting them to TensorRT engine using torch2trt.\n",
    "### Thus using mobilenet would be only applicable for pytorch simulation but not for TensorRT.\n",
    "\n",
    "# vgg11\n",
    "# googlenet\n",
    "# inception_v3\n",
    "# resnet18, resnet34, resnet50, resnet101; 18, 34, 50, ... no of layers of network\n",
    "# densenet121, densenet161, densenet169, densenet201; 121, 161, ... no of layers of network\n",
    "# shufflenet_v2_x2_0, shufflenet_v2_x1_5, shufflenet_v2_x1_0, shufflenet_v2_x0_5 xy_y: factor determines depth of tensor (no. of channels or no. of filters of each stage)\n",
    "# mnasnet1_3, mnasnet1_0, mnasnet0_75, mnasnet0_5; y_y: MnasNet_A1 model, depth multiplier determines depth of tensor (no. of channel, no. of filters)\n",
    "# mobilenet, mobilenet_v2, mobilenet_v3_large, mobilenet_v3_small\n",
    "# efficientnet_b0~b5, bx: using different compound coefficient phi\n",
    "# vit_x_xx : x: b, l, h; xx: 16, 32, 14(for h only))\n",
    "\n",
    "TRAIN_MODEL = 'resnet18'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Neural Network Model \n",
    "\n",
    "1. In a process called $transfer$  $learning$, we can repurpose a pre-trained model (trained on millions of images) for a new task that has possibly much less data available.\n",
    "More Details on Transfer Learning: https://www.youtube.com/watch?v=yofjFQddwHE \n",
    "\n",
    "2. The $transfer$  $learning$ will be used for training the model for road following  simulation.\n",
    "\n",
    "3. You can use the models (the parameter setting TRAIN_MODEL above) available in PyTorch TorchVision package, such as resnet18, resnet34, resnet50, resnet101, mobilenet_v2, vgg11, mobilenet_v3_large, inception_v3, efficientnet_b4, googlenet, vit, densenet, shufflenet..., etc.\n",
    "\n",
    "4. Before you use the pre-trained pytorch model for transfer learning, you should modify the classifier nodes parameters of the selected neural model architecture. \n",
    "\n",
    "5. The modification should be done through modifying the function `load_tune_pth_model` in `model_selection.py` which is located in the director jetbot/utils/. \n",
    "> e.g., ResNet model has fully connected (fc) final layer with 512 as in_features, and we will be training for regression thus the out_features is set to 2.\n",
    "\n",
    "6. Please refer the information in the following websites for modifying pytorch pre-trained models:\n",
    "* classifier : https://pytorch.org/vision/0.10/models.html#classification\n",
    "* github : https://github.com/pytorch/vision/tree/release/0.11/torchvision/models\n",
    "* More details on ResNet-18 and other variants: https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "\n",
    "7. After modifying the classifier, load the model below:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "%cd \"../../jetbot/utils\"\n",
    "from model_selection import load_tune_pth_model\n",
    "model, model_type, preprocess_wrap = load_tune_pth_model(pth_model_name=TRAIN_MODEL, pretrained=True) \n",
    "# preprocess_wrap = [preprocess, classifier_preprocess] or None   # refer to load_tune_pth_model in model_selection.py\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and extract data\n",
    "\n",
    "1. Before you start, you need to make sure the zip data file for training has been created by using ``data_collection.ipynb`` notebook and stored in the jetbot.\n",
    "\n",
    "2. you should upload the zip data file for training to the PC directory set by `dir_depo` in the cell below.ã€€The default `dir_depo` is `D:\\\\AI_Lecture_Demos\\\\Data_Repo\\\\Cuterbot_Repo`. All the training will be worked on this directory. If you want to use another directory, you may set this variable. \n",
    "\n",
    "> If you're training on the JetBot that you collected data on, you can skip this step!\n",
    "\n",
    "3. After training is finished, the trained model will be stored in the directory `dir_depo`.\n",
    "\n",
    "4. The data set file name (variable name: `training_datafile`) should set to the exact same file name that you used in ``data_collection.ipynb``, which would have format `road_following_{DATASET_DIR}_{timestr()}.zip`. In this example, the variable `training_datafile` used here is `dataset_xy_0916_1.zip`.\n",
    "\n",
    "5. You should then extract the dataset for training by calling the command below:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# !unzip -q road_following.zip\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# The dir_depo parameter can be set as you required: \n",
    "dir_depo = 'D:\\\\Temp\\\\Cuterbot_Repo'\n",
    "# dir_depo = 'D:\\\\AI_Lecture_Demos\\\\Data_Repo\\\\Cuterbot_2004_Repo'\n",
    "os.makedirs(dir_depo, exist_ok=True)\n",
    "# dir_depo = os.getcwd()\n",
    "training_datafile = 'road_following_dataset_xy_2024-12-22_13-18-05.zip'  # check the data file is loaded to dir_depo\n",
    "\n",
    "with ZipFile(os.path.join(dir_depo, training_datafile), 'r') as zObject:\n",
    "    zObject.extractall(path=dir_depo)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a folder named ``dataset_xy`` appear in the file directory as set in variable `dir_depo`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset Instance\n",
    "\n",
    "Here we create a custom ``torch.utils.data.Dataset`` implementation, which implements the ``__len__`` and ``__getitem__`` functions.  This class\n",
    "is responsible for loading images and parsing the x, y values from the image filenames.  Because we implement the ``torch.utils.data.Dataset`` class,\n",
    "we can use all of the torch data utilities :)\n",
    "\n",
    "We hard coded some transformations (like color jitter) into our dataset.  We made random horizontal flips optional (in case you want to follow a non-symmetric path, like a road\n",
    "where we need to 'stay right').  If it doesn't matter whether your robot follows some convention, you could enable flips to augment the dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_x(path, width):\n",
    "    \"\"\"Gets the x value from the image filename\"\"\"\n",
    "    # print(path.split(\"_\")[1])\n",
    "    return (float(int(path.split(\"_\")[1])) - width/2) / (width/2)\n",
    "\n",
    "def get_y(path, height):\n",
    "    \"\"\"Gets the y value from the image filename\"\"\"\n",
    "    # print(path.split(\"_\")[2])\n",
    "    return (float(int(path.split(\"_\")[2])) - height/2) / (height/2)\n",
    "\n",
    "class XYDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, directory, preprocess, random_hflips=False):\n",
    "        self.directory = directory\n",
    "        self.random_hflips = random_hflips\n",
    "        self.preprocess = preprocess     # use weights.transform() for torchvision with version > 0.13\n",
    "        self.image_paths = glob.glob(os.path.join(self.directory, '*.jpg'))\n",
    "        self.color_jitter = transforms.ColorJitter(0.3, 0.3, 0.3, 0.3)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        \n",
    "        image = PIL.Image.open(image_path)\n",
    "        width, height = image.size\n",
    "        # print(width, height)\n",
    "        x = float(get_x(os.path.basename(image_path), width))\n",
    "        y = float(get_y(os.path.basename(image_path), height))\n",
    "        # print(x, y)\n",
    "        if float(np.random.rand(1)) > 0.5:\n",
    "            image = transforms.functional.hflip(image)\n",
    "            x = -x\n",
    "        \n",
    "        image = self.color_jitter(image)\n",
    "        image = self.preprocess(image)\n",
    "            \n",
    "        '''\n",
    "        if TRAIN_MODEL == 'inception_v3':\n",
    "            image = transforms.functional.resize(image, (299, 299))\n",
    "        else:\n",
    "            image = transforms.functional.resize(image, (224, 224))\n",
    "        image = transforms.functional.to_tensor(image)\n",
    "        image = image.numpy()[::-1].copy()\n",
    "        image = torch.from_numpy(image)\n",
    "        image = transforms.functional.normalize(image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        '''\n",
    "        return image, torch.tensor([x, y]).float()\n",
    "    \n",
    "dataset = XYDataset(os.path.join(dir_depo, 'dataset_xy'), preprocess_wrap[0], random_hflips=False)  # preprocess_wrap[0] : preprocess for model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train and test sets\n",
    "Once we read dataset, we will split data set in train and test sets. In this example we split train and test a 90%-10%. The test set will be used to verify the accuracy of the model we train."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test_percent = 0.1\n",
    "num_test = int(test_percent * len(dataset))\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - num_test, num_test])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data loaders to load data in batches\n",
    "\n",
    "We use ``DataLoader`` class to load data in batches, shuffle data and allow using multi-subprocesses. In this example we use batch size of 64. Batch size will be based on memory available with your GPU and it can impact accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processor used for training\n",
    "1. You may use CPU or GPU for training the model by checking the check widget below.\n",
    "2. If you use GPU for training, the model you selected (in \"TRAIN_MODEL\") will be transferred for execution on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ** you may use CPU or GPU for training\n",
    "processor = 'GPU'\n",
    "if processor == 'GPU':\n",
    "    device = torch.device('cuda')\n",
    "    print(\"torch cuda version : \", torch.version.cuda)\n",
    "    print(\"cuda is available for pytorch: \", torch.cuda.is_available())    \n",
    "elif processor == 'CPU':\n",
    "    device = torch.device('cpu')\n",
    "model = model.float()\n",
    "model = model.to(device, dtype=torch.float)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Learning Algorithm for training Neural Network Model:\n",
    "1. You can use the following learning algorithms (the parameter setting `TRAIN_METHOD` above) for training a model\n",
    "\"Adam\", \"SGD\", \"ASGD\", \"Adadelta\", \"RAdam\"; the parameters lr=0.01, momentum=0.92 is needed for SGD.\n",
    "* Reference web site : https://pytorch.org/docs/stable/optim.html#algorithms"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# *** refererence : https://pytorch.org/docs/stable/optim.html#algorithms\n",
    "# use the following learning algorithms for evaluation\n",
    "# \"Adam\", \"SGD\", \"ASGD\", \"Adadelta\", \"RAdam\"; the parameters lr=0.01, momentum=0.92 is needed for SGD\n",
    "TRAIN_METHOD = 'Adam'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Regression:\n",
    "\n",
    "1. The training record is stored in the directory set by variable `dir_training_records` in the cell below.\n",
    "2. The trained model for road following is saved in directory `DIR_RC_MODEL_REPO` with file name `BEST_MODEL_PATH` in the cell below, and the `torch_model_tbl.csv` in the directory `DIR_MODEL_REPO` will be updated accordingly. \n",
    "3. We train for 70 epochs and save the best model if the loss is reduced.\n",
    "4. The parameters or variables you may be modified as you want."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%cd \"../../jetbot/utils\"\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from training_profile import * \n",
    "\n",
    "dir_training_records = os.path.join(dir_depo, processor, 'training records', \"road_following\", TRAIN_MODEL)\n",
    "os.makedirs(dir_training_records, exist_ok=True)\n",
    "\n",
    "DIR_MODEL_REPO = os.path.join(dir_depo, processor, 'model_repo')\n",
    "os.makedirs(DIR_MODEL_REPO, exist_ok=True)\n",
    "DIR_RC_MODEL_REPO = os.path.join(DIR_MODEL_REPO, 'road_following')\n",
    "os.makedirs(DIR_RC_MODEL_REPO, exist_ok=True)\n",
    "\n",
    "# BEST_MODEL_PATH = 'best_steering_model_xy.pth'\n",
    "BEST_MODEL_NAME = \"best_steering_model_xy_\" + TRAIN_MODEL\n",
    "BEST_MODEL_PATH = os.path.join(DIR_RC_MODEL_REPO, BEST_MODEL_NAME + \".pth\")\n",
    "MODEL_PREPROCESS_PATH = os.path.join(DIR_RC_MODEL_REPO, BEST_MODEL_NAME + \"_preprocess.pth\")\n",
    "\n",
    "NUM_EPOCHS = 70\n",
    "best_loss = 1e9\n",
    "\n",
    "optimizer = getattr(optim, TRAIN_METHOD)(model.parameters(), weight_decay=0)\n",
    "# optimizer = getattr(optim, TRAIN_METHOD)(model.parameters(), lr=0.01, momentum=0.95)\n",
    "\n",
    "loss_data = []\n",
    "lt_epoch = []  # learning time per epoch\n",
    "lt_sample = []  # learning time per epoch\n",
    "\n",
    "print(\"start training model ----- %s -----\" % TRAIN_MODEL)\n",
    "\n",
    "batch_size = len(train_loader)\n",
    "pbar_overall_format = \"{desc} {percentage:.2f}% | {bar} | elapsed: {elapsed}; estimated to finish: {remaining}\"\n",
    "pbar_overall = tqdm(total=100, bar_format = pbar_overall_format)\n",
    "show_batch_progress = False   # Set True if need to show the batch learning progress in an epoch\n",
    "show_training_plot = False  # Set True if need to show the convergent profile during training\n",
    "\n",
    "best_loss = None\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_epoch = time.time()\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    if show_batch_progress:\n",
    "        pbar_batch = tqdm(train_loader, total = batch_size)\n",
    "    else:\n",
    "        pbar_batch = iter(train_loader)\n",
    "    for index, (images, labels) in enumerate(pbar_batch):\n",
    "        start_sample = time.time()\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        if TRAIN_MODEL == 'inception_v3':\n",
    "            outputs = model(images)\n",
    "            loss_main = F.mse_loss(outputs.logits, labels, reduction='mean')\n",
    "            loss_aux = F.mse_loss(outputs.aux_logits, labels, reduction='mean')\n",
    "            loss = loss_main + 0.3 * loss_aux  # weighting aux with 0.3 is the same as that stated in paper\n",
    "            \n",
    "        elif TRAIN_MODEL == 'googlenet':\n",
    "            outputs = model(images)\n",
    "            loss_main = F.mse_loss(outputs.logits, labels, reduction='mean')\n",
    "            loss_aux1 = F.mse_loss(outputs.aux_logits1, labels, reduction='mean')\n",
    "            loss_aux2 = F.mse_loss(outputs.aux_logits2, labels, reduction='mean')\n",
    "            loss = loss_main + 0.3 * loss_aux1 + 0.3 * loss_aux2  # weighting aux with 0.3 is the same as that stated in paper\n",
    "            \n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = F.mse_loss(outputs, labels, reduction='mean')\n",
    "            \n",
    "        train_loss += float(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        end_sample = time.time()\n",
    "        lt_sample.append(end_sample - start_sample)\n",
    "        \n",
    "        pbar_overall.update(round(100/(NUM_EPOCHS*batch_size), 2))\n",
    "        pbar_overall.set_description(desc = f'Overall progress - Epoch [{epoch+1}/{NUM_EPOCHS}]')\n",
    "        pbar_overall.set_postfix(best_loss = best_loss, train_loss = train_loss/(index+1))\n",
    "               \n",
    "        if show_batch_progress:\n",
    "            pbar_batch.set_description(desc = f'Progress in the epoch {epoch+1} ')\n",
    "            pbar_batch.set_postfix(mean_batch_loss = train_loss/(index+1))\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    for images, labels in iter(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = F.mse_loss(outputs, labels)\n",
    "        test_loss += float(loss)\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    end_epoch = time.time()\n",
    "    lt_epoch.append(end_epoch - start_epoch)\n",
    "      \n",
    "    if best_loss == None or test_loss < best_loss :\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "        best_loss = test_loss\n",
    "    \n",
    "    loss_data.append([train_loss, test_loss])\n",
    "    \n",
    "# function plot_loss(loss_data, best_loss, no_epoch, dir_training_records, train_model, train_method) is in jetbot.utils\n",
    "    if epoch == NUM_EPOCHS:\n",
    "        show_training_plot=True\n",
    "    plot_loss(loss_data=loss_data, best_loss=best_loss, no_epoch=NUM_EPOCHS,\n",
    "              dir_training_records=dir_training_records, # the directory stored training records\n",
    "              train_model=TRAIN_MODEL, train_method=TRAIN_METHOD, processor=processor, # this 2 parameters are for plot title only\n",
    "              show_training_plot=show_training_plot)\n",
    "\n",
    "overall_time = pbar_overall.format_dict[\"elapsed\"]\n",
    "# function lt_plot(lt_epoch, lt_sample, dir_training_records, train_model, train_method) is in jetbot.utils\n",
    "lt_plot(lt_epoch=lt_epoch, lt_sample=lt_sample, overall_time=overall_time,\n",
    "        dir_training_records=dir_training_records, # the directory stored training records\n",
    "        train_model=TRAIN_MODEL, train_method=TRAIN_METHOD, processor=processor) # this 2 parameters are for plot title only\n",
    "\n",
    "# save the preprocess module (preprocess_wrap[1]) created from model_selection.py module\n",
    "torch.save(preprocess_wrap[1].state_dict(), MODEL_PREPROCESS_PATH)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the file path of the trained model\n",
    "The file path of the trained model (model_path) and the associated preprocess module (preprocess_path) are stored in `torch_model_tbl.csv` ubder the directory `DIR_MODEL_REPO`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_file = os.path.join(DIR_MODEL_REPO, 'torch_model_tbl.csv')\n",
    "model_path = \"./road_following/\" + BEST_MODEL_NAME + \".pth\"\n",
    "preprocess_path = \"./road_following/\" + BEST_MODEL_NAME + \"_preprocess.pth\"\n",
    "if os.path.isfile(df_file):\n",
    "    df = pd.read_csv(df_file, header=None)\n",
    "else:\n",
    "    df = pd.DataFrame()\n",
    "df = df._append([[\"classifier\", model_type, model_path, preprocess_path]], ignore_index = False)\n",
    "df = df.drop_duplicates()\n",
    "df.to_csv(df_file, header=False, index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print('Training is completed! \\n you can close the figures by restart the kernel!')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Once the model is trained, it will generate a pytorch model file ``best_steering_model_xy_<TRAIN_MODEL>.pth`` and update the file `torch_model_tbl.csv` in `PC` directories as set in `DIR_RC_MODEL_REPO` and `DIR_MODEL_REPO`, respectively; then you have to  copy these 2 files to the `jetbot` directories `model_repo\\road_following` and `model_repo`, respectively. Then you can use them for inferencing by starting `jupyter lab of jetbot` and running `live_demo_light.ipynb`.\n",
    "2. To run the model with Nvidia TensorRT support, you have to create a TensorRT engine by using `live_demo_build_trt.ipynb` which will covert the  generated pytorch model file to a TensorRT engine; then you can execute `live_demo_light_trt.ipynb` with the created engine.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
