{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Road Follower - Train Model\n",
    "\n",
    "In this notebook we will train a neural network to take an input image, and output a set of x, y values corresponding to a target.\n",
    "\n",
    "We will be using PyTorch deep learning framework to train ResNet18 neural network architecture model for road follower application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute this script as followings:\n",
    "# set project interpreter to PC python of version 3.8 pr above, and then\n",
    "# run jupyter lab in command window \n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import glob\n",
    "import PIL.Image\n",
    "import os\n",
    "import numpy as np\n",
    "import ipywidgets\n",
    "\n",
    "\n",
    "TRAIN_MODEL = \"efficientnet_b4\"  # resnet18, resnet34, resnet50, resnet101, mobilenet_v2, vgg11, mobilenet_v3_large, inception_v3, efficientnet_b4, googlenet\n",
    "\n",
    "# *** refererence : https://pytorch.org/docs/stable/optim.html#algorithms\n",
    "# use the following learning algorithms for evaluation\n",
    "TRAIN_METHOD = \"Adam\"  # \"Adam\", \"SGD\", \"ASGD\", \"Adadelta\", \"RAdam\"; the parameters lr=0.01, momentum=0.92 is needed for SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and extract data\n",
    "\n",
    "Before you start, you should upload the zip file that you created in the ``data_collection.ipynb`` notebook on the robot. \n",
    "\n",
    "> If you're training on the JetBot you collected data on, you can skip this!\n",
    "\n",
    "You should then extract this dataset by calling the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip -q road_following.zip\n",
    "from zipfile import ZipFile\n",
    "dir_depo = 'D:\\\\AI_Lecture_Demos\\\\Data_Repo\\\\Cuterbot_2004_Repo'\n",
    "os.makedirs(dir_depo, exist_ok=True)\n",
    "# dir_depo = os.getcwd()\n",
    "training_datafile = 'dataset_xy_0916_1.zip'  # check the data file is loaded to dir_depo\n",
    "\n",
    "with ZipFile(os.path.join(dir_depo, training_datafile), 'r') as zObject:\n",
    "    zObject.extractall(path=dir_depo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a folder named ``dataset_xy`` appear in the file directory dir_depo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset Instance\n",
    "\n",
    "Here we create a custom ``torch.utils.data.Dataset`` implementation, which implements the ``__len__`` and ``__getitem__`` functions.  This class\n",
    "is responsible for loading images and parsing the x, y values from the image filenames.  Because we implement the ``torch.utils.data.Dataset`` class,\n",
    "we can use all of the torch data utilities :)\n",
    "\n",
    "We hard coded some transformations (like color jitter) into our dataset.  We made random horizontal flips optional (in case you want to follow a non-symmetric path, like a road\n",
    "where we need to 'stay right').  If it doesn't matter whether your robot follows some convention, you could enable flips to augment the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x(path, width):\n",
    "    \"\"\"Gets the x value from the image filename\"\"\"\n",
    "    return (float(int(path.split(\"_\")[1])) - width/2) / (width/2)\n",
    "\n",
    "def get_y(path, height):\n",
    "    \"\"\"Gets the y value from the image filename\"\"\"\n",
    "    return (float(int(path.split(\"_\")[2])) - height/2) / (height/2)\n",
    "\n",
    "class XYDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, directory, random_hflips=False):\n",
    "        self.directory = directory\n",
    "        self.random_hflips = random_hflips\n",
    "        self.image_paths = glob.glob(os.path.join(self.directory, '*.jpg'))\n",
    "        self.color_jitter = transforms.ColorJitter(0.3, 0.3, 0.3, 0.3)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        \n",
    "        image = PIL.Image.open(image_path)\n",
    "        width, height = image.size\n",
    "        x = float(get_x(os.path.basename(image_path), width))\n",
    "        y = float(get_y(os.path.basename(image_path), height))\n",
    "      \n",
    "        if float(np.random.rand(1)) > 0.5:\n",
    "            image = transforms.functional.hflip(image)\n",
    "            x = -x\n",
    "        \n",
    "        image = self.color_jitter(image)\n",
    "        if TRAIN_MODEL == 'inception_v3':\n",
    "            image = transforms.functional.resize(image, (299, 299))\n",
    "        else:\n",
    "            image = transforms.functional.resize(image, (224, 224))\n",
    "        image = transforms.functional.to_tensor(image)\n",
    "        image = image.numpy()[::-1].copy()\n",
    "        image = torch.from_numpy(image)\n",
    "        image = transforms.functional.normalize(image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        \n",
    "        return image, torch.tensor([x, y]).float()\n",
    "    \n",
    "dataset = XYDataset(os.path.join(dir_depo, 'dataset_xy'), random_hflips=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train and test sets\n",
    "Once we read dataset, we will split data set in train and test sets. In this example we split train and test a 90%-10%. The test set will be used to verify the accuracy of the model we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_percent = 0.1\n",
    "num_test = int(test_percent * len(dataset))\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - num_test, num_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data loaders to load data in batches\n",
    "\n",
    "We use ``DataLoader`` class to load data in batches, shuffle data and allow using multi-subprocesses. In this example we use batch size of 64. Batch size will be based on memory available with your GPU and it can impact accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Neural Network Model \n",
    "\n",
    "We use ResNet-18 model available on PyTorch TorchVision. \n",
    "\n",
    "In a process called transfer learning, we can repurpose a pre-trained model (trained on millions of images) for a new task that has possibly much less data available.\n",
    "\n",
    "\n",
    "More details on ResNet-18 and other variants: https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "\n",
    "More Details on Transfer Learning: https://www.youtube.com/watch?v=yofjFQddwHE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.resnet18(pretrained=True)\n",
    "model = getattr(models, TRAIN_MODEL)()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet model has fully connect (fc) final layer with 512 as ``in_features`` and we will be training for regression thus ``out_features`` as 2\n",
    "\n",
    "Finally, we transfer our model for execution on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch cuda version :  11.3\n",
      "cuda is available for pytorch:  True\n"
     ]
    }
   ],
   "source": [
    "# ----- modify last layer for classification, and the model used in notebook should be modified as well.\n",
    "# Referring the following information can pytorch model web site:\n",
    "# 1. classifier : https://pytorch.org/vision/0.10/models.html#classification\n",
    "# 2. github : https://github.com/pytorch/vision/tree/release/0.11/torchvision/models\n",
    "\n",
    "if TRAIN_MODEL == 'mobilenet_v3_large':  # MobileNet\n",
    "    model.classifier[3] = torch.nn.Linear(model.classifier[3].in_features, 2)  # for mobilenet_v3 model. must add the block expansion factor 4\n",
    "\n",
    "elif TRAIN_MODEL == 'mobilenet_v2':\n",
    "    model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 2)  # for mobilenet_v2 model. must add the block expansion factor 4\n",
    "\n",
    "elif TRAIN_MODEL == 'vgg11': # VGGNet\n",
    "    model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, 2)  # for VGG model. must add the block expansion factor 4\n",
    "\n",
    "elif 'resnet' in TRAIN_MODEL: # ResNet\n",
    "    model.fc = torch.nn.Linear(model.fc.in_features, 2)  # for resnet model must add the block expansion factor 4\n",
    "    # model.fc = torch.nn.Linear(512, 2)\n",
    "\n",
    "elif TRAIN_MODEL == 'inception_v3':   # Inception_v3\n",
    "    model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
    "    if model.aux_logits:\n",
    "        model.AuxLogits.fc = torch.nn.Linear(model.AuxLogits.fc.in_features, 2)\n",
    "\n",
    "elif 'efficientnet' in TRAIN_MODEL:   # efficientnet\n",
    "    model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 2)  # for efficientnet_b1 \n",
    "\n",
    "# ** you may use CPU or GPU for training\n",
    "processor = 'GPU'\n",
    "if processor == 'GPU':\n",
    "    device = torch.device('cuda')\n",
    "    print(\"torch cuda version : \", torch.version.cuda)\n",
    "    print(\"cuda is available for pytorch: \", torch.cuda.is_available())    \n",
    "elif processor == 'CPU':\n",
    "    device = torch.device('cpu')\n",
    "model = model.float()\n",
    "model = model.to(device, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Regression:\n",
    "\n",
    "We train for 70 epochs and save best model if the loss is reduced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\Courses\\Course_Demos\\NTUC\\AI_Lecture_Demos\\Cuterbot_Demo\\jetbot\\utils\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e88adb138540028c647fb738590899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " 0.00% |            | elapsed: 00:00; estimated to finish: ?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean learning time per epoch: 128.435 s, maximum epoch learning time: 142.188 s, minimum epoch learning time: 119.031 s\n",
      "mean learning time per sample: 0.966 s, maximum sample learning time: 1.406 s, minimum sample learning time: 0.625 s\n",
      "Training is completed! \n",
      " you can close the figures by restart the kernel!\n"
     ]
    }
   ],
   "source": [
    "%cd \"../../jetbot/utils\"\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from training_profile import *\n",
    "\n",
    "dir_training_records = os.path.join(dir_depo, 'training records', processor, TRAIN_MODEL)\n",
    "os.makedirs(dir_training_records, exist_ok=True)\n",
    "\n",
    "DIR_MODEL_REPO = os.path.join(dir_depo, 'model_repo', processor)\n",
    "os.makedirs(DIR_MODEL_REPO, exist_ok=True)    \n",
    "# BEST_MODEL_PATH = 'best_steering_model_xy.pth'\n",
    "BEST_MODEL_PATH = os.path.join(DIR_MODEL_REPO, \"best_steering_model_xy_\" + TRAIN_MODEL + \".pth\")\n",
    "\n",
    "NUM_EPOCHS = 70\n",
    "best_loss = 1e9\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters())\n",
    "optimizer = getattr(optim, TRAIN_METHOD)(model.parameters(), weight_decay=0)\n",
    "# optimizer = getattr(optim, TRAIN_METHOD)(model.parameters(), lr=0.01, momentum=0.95)\n",
    "\n",
    "loss_data = []\n",
    "lt_epoch = []  # learning time per epoch\n",
    "lt_sample = []  # learning time per epoch\n",
    "\n",
    "batch_size = len(train_loader)\n",
    "pbar_overall_format = \"{desc} {percentage:.2f}% | {bar} | elapsed: {elapsed}; estimated to finish: {remaining}\"\n",
    "pbar_overall = tqdm(total=100, bar_format = pbar_overall_format)\n",
    "show_batch_progress = False   # Set True if need to show the batch learning progress in an epoch\n",
    "show_training_plot = False  # Set True if need to show the converbent profile during training\n",
    "\n",
    "\n",
    "best_loss = None\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_epoch = time.process_time()\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    if show_batch_progress:\n",
    "        pbar_batch = tqdm(train_loader, total = batch_size)\n",
    "    else:\n",
    "        pbar_batch = iter(train_loader)\n",
    "    for index, (images, labels) in enumerate(pbar_batch):\n",
    "        start_sample = time.process_time()\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        if TRAIN_MODEL == 'inception_v3':\n",
    "            outputs = model(images)\n",
    "            loss_main = F.mse_loss(outputs.logits, labels, reduction='mean')\n",
    "            loss_aux = F.mse_loss(outputs.aux_logits, labels, reduction='mean')\n",
    "            loss = loss_main + 0.3 * loss_aux\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = F.mse_loss(outputs, labels, reduction='mean')\n",
    "        train_loss += float(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        end_sample = time.process_time()\n",
    "        lt_sample.append(end_sample - start_sample)\n",
    "        \n",
    "        pbar_overall.update(round(100/(NUM_EPOCHS*batch_size), 2))\n",
    "        pbar_overall.set_description(desc = f'Overall progress - Epoch [{epoch+1}/{NUM_EPOCHS}]')\n",
    "        pbar_overall.set_postfix(best_loss = best_loss, train_loss = train_loss/(index+1))\n",
    "               \n",
    "        if show_batch_progress:\n",
    "            pbar_batch.set_description(desc = f'Progress in the epoch {epoch+1} ')\n",
    "            pbar_batch.set_postfix(mean_batch_loss = train_loss/(index+1))               \n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    for images, labels in iter(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = F.mse_loss(outputs, labels)\n",
    "        test_loss += float(loss)\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    end_epoch = time.process_time()\n",
    "    lt_epoch.append(end_epoch - start_epoch)\n",
    "      \n",
    "    if best_loss == None or test_loss < best_loss :\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "        best_loss = test_loss\n",
    "    \n",
    "    loss_data.append([train_loss, test_loss])\n",
    "    \n",
    "# function plot_loss(loss_data, best_loss, no_epoch, dir_training_records, train_model, train_method) is in jetbot.utils\n",
    "    if epoch == NUM_EPOCHS:\n",
    "        show_training_plot=True    \n",
    "    plot_loss(loss_data=loss_data, best_loss=best_loss, no_epoch=NUM_EPOCHS,\n",
    "              dir_training_records=dir_training_records, # the directory stored training records\n",
    "              train_model=TRAIN_MODEL, train_method=TRAIN_METHOD, processor=processor, # this 2 parameters are for plot title only\n",
    "              show_training_plot=show_training_plot)\n",
    "\n",
    "# function lt_plot(lt_epoch, lt_sample, dir_training_records, train_model, train_method) is in jetbot.utils\n",
    "lt_plot(lt_epoch=lt_epoch, lt_sample=lt_sample, \n",
    "        dir_training_records=dir_training_records, # the directory stored training records\n",
    "        train_model=TRAIN_MODEL, train_method=TRAIN_METHOD, processor=processor) # this 2 parameters are for plot title only\n",
    "\n",
    "print('Training is completed! \\n you can close the figures by restart the kernel!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, it will generate ``best_steering_model_xy_<TRAIN_MODEL>.pth`` file which you can use for inferencing in the live demo notebook.\n",
    "\n",
    "If you trained on a different machine other than JetBot, you'll need to upload this to the JetBot to the ``road_following`` example folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
