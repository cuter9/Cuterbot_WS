{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road Following - Build TensorRT model for live demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will optimize the model we trained using TensorRT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. This script is only for the models trained with `train_model_PC.ipynb`. If the model is trained with `train_model.ipynb`, you should use live_demo_build_trt_o.ipynb.\n",
    "2. Before executing this script, you should copy the whole directory and the trained model files saved inside it to `Home/model_repo`, where HOME is the home directory of your Jetson Nano (here, we assume HOME=/home/cuterbot). The trained model files is saved in the directory variable \"DIR_RC_MODEL_REPO\" you set in `train_model_PC.ipynb`, which should be in the PC used for training.\n",
    "3. The pytorch model will be uploaded after you selected the model from the selection widget in the cell below.\n",
    "> Note: Please make sure the file has been uploaded fully from the log console of jupyter lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code below to initialize the PyTorch model. This should look very familiar from the training notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/model_repo\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from jetbot.utils import model_selection\n",
    "import os \n",
    "import ipywidgets.widgets as widgets\n",
    "from ipywidgets.widgets import Box, HBox, VBox, Layout, Label, Output\n",
    "import traitlets\n",
    "\n",
    "# The path of trt models is 'MODEL_REPO_DIR_DOCKER' which is set in /jetbot/utils/model_selection.py,\n",
    "# which may be modified if you change the file path of trt models, 'MODEL_REPO_DIR_DOCKER' or dir_model_repo.\n",
    "\n",
    "dir_model_repo = os.environ['MODEL_REPO_DIR_DOCKER']\n",
    "print(\"The model repository (MODEL_REPO_DIR_DOCKER) in jetbot : %s \" % dir_model_repo)\n",
    "\n",
    "# pth_ms = model_selection(core_library = \"Pytorch\")  # if 'MODEL_REPO_DIR_DOCKER' is used.\n",
    "pth_ms = model_selection(core_library = \"Pytorch\", dir_model_repo=dir_model_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_ms.model_function = \"classifier\"\n",
    "\n",
    "model_type_widget = widgets.Select(options=pth_ms.model_type_list, value=pth_ms.model_type_list[0],\n",
    "                                      description='Model Type:')\n",
    "traitlets.dlink((pth_ms, 'model_type_list'), (model_type_widget, 'options'))\n",
    "traitlets.dlink((model_type_widget, 'value'), (pth_ms, 'model_type'))\n",
    "\n",
    "model_path_widget = widgets.Select(options=pth_ms.model_path_list, description='Model Path:',\n",
    "                                      layout=Layout(width='65%'))\n",
    "traitlets.dlink((pth_ms, 'model_path_list'), (model_path_widget, 'options'))\n",
    "traitlets.dlink((model_path_widget, 'value'), (pth_ms, 'model_path'))\n",
    "\n",
    "path_preprocess = pth_ms.preprocess_path  # the preprocess for torch model is used for trt model\n",
    "# print('preprocess path:', path_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load the pytoch model and the trained weights from the model_path (e.g.``best_steering_model_xy_<<pth_model_name>>.pth``) file that you uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = Output(layout={'border': '2px solid black'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "@out.capture()\n",
    "def load_trained_model(model_path, pth_model_name):\n",
    "    from jetbot.utils.model_selection import load_tune_pth_model\n",
    "\n",
    "    print(\"start load trained model -- \\n model name : \", pth_model_name, '\\n model path: ', model_path)\n",
    "\n",
    "    model, model_type, preprocess_wrap = load_tune_pth_model(pth_model_name=pth_model_name, pretrained=False)  \n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    # Currently, the model weights are located on the CPU memory execute the code below to transfer to the GPU device. \n",
    "    model.to(device).eval().half()\n",
    "    # model = model.cuda().eval().half()\n",
    "\n",
    "    return model_type, model, preprocess_wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorRT conversion:\n",
    "\n",
    "1. If you are running with docker container, you may not need to do the following installation.\n",
    "> Note: If your setup does not have `torch2trt` installed, you need to first install `torch2trt` by executing the following in the console.\n",
    "```bash\n",
    "    cd $HOME\n",
    "    git clone https://github.com/NVIDIA-AI-IOT/torch2trt\n",
    "    cd torch2trt\n",
    "    sudo python3 setup.py install\n",
    "```\n",
    "> Convert and optimize the model using torch2trt for faster inference with TensorRT. Please see the [torch2trt](https://github.com/NVIDIA-AI-IOT/torch2trt) readme for more details.\n",
    "> This optimization process can take a couple of minutes to complete.\n",
    "\n",
    "2. After finishing TensorRT engine conversion, the created TensorRT model engines will be stored in same directory you specified to store the step 2 above (`Home/model_repo/road_following`), and the mata data file 'trt_model_tbl.csv' will be updated in directory `$Home/model_repo`.\n",
    "3. Then, you can execute the `live_demo_light_trt.ipynb` to simulate the road following function with the converted TensorRT engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@out.capture()\n",
    "def trt_conversion(pth_model_name, model, preprocess):\n",
    "    from torch2trt import torch2trt\n",
    "    import tensorrt as trt # Logger : ERROR, INFO, VERBOSE, WARNING\n",
    "    \n",
    "    print(\"start building TRT model -- \")\n",
    "\n",
    "    preprocess.to(device).eval().half()\n",
    "    data = torch.zeros((1, 3, 224, 224)).to(device).half()\n",
    "    data = preprocess(data).to(device).half()\n",
    "    # data = preprocess(data).cuda().half()\n",
    "    \n",
    "    '''\n",
    "    if pth_model_name == 'inception_v3':\n",
    "        data = torch.zeros((1, 3, 299, 299)).cuda().half()   # inception_v3\n",
    "    else:\n",
    "        data = torch.zeros((1, 3, 224, 224)).cuda().half()  # resnet\n",
    "    '''   \n",
    "    model_trt = torch2trt(model, [data], fp16_mode=True, log_level=trt.Logger.VERBOSE)\n",
    "    \n",
    "    return model_trt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the optimized model using the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the optimized model using the cell below\n",
    "@out.capture()\n",
    "def save_trt_model(pth_model_name, model_type, model_trt, path_trt_model, path_trt_model_preprocess):\n",
    "    import pandas as pd\n",
    "    print(\"saving built trt model  --\")\n",
    "    \n",
    "    torch.save(model_trt.state_dict(), path_trt_model)\n",
    "\n",
    "    df_file = os.path.join(dir_model_repo, 'trt_model_tbl.csv')\n",
    "    if os.path.isfile(df_file):\n",
    "        df = pd.read_csv(df_file, header=None)\n",
    "    else:\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "    path_trt_model_tbl = path_trt_model.replace(dir_model_repo, '.')\n",
    "    path_trt_model_preprocess_tbl = path_trt_model_preprocess.replace(dir_model_repo, '.')\n",
    "    df = df.append([[\"classifier\", model_type, path_trt_model_tbl, path_trt_model_preprocess_tbl]], ignore_index = False)\n",
    "    df = df.drop_duplicates()\n",
    "    df.to_csv(df_file, header=False, index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e381a741b64f7082b0506cf4d84977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Select(description='Model Type:', options=('DenseNet', 'EfficientNet', 'GoogleNet', 'InceptionNâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4a6d71a45746d4aeb64af3f7d1d735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='OK', icon='start', style=ButtonStyle(button_color='lightblue'), tooltip='Click to start')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df158e4093544e1a4baf336aff085b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='2px solid black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@out.capture()\n",
    "def start_trt_conversion(change):\n",
    "    from jetbot.utils import tv_classifier_preprocess\n",
    "    button_OK.disabled=True\n",
    "    \n",
    "    all_model = True     # set True to convert all pytorch models\n",
    "    re_convert = True  # wether to do trt conversion of conerted trt model\n",
    "    \n",
    "    list_model_path = []   # [[torch model path, model preprocess path], ....]\n",
    "    if not all_model:\n",
    "        list_model_path.append([model_path_widget.value, pth_ms.preprocess_path])\n",
    "    else:\n",
    "        import pandas as pd\n",
    "        from jetbot.utils.model_selection import HEAD_LIST\n",
    "        df = pd.read_csv(os.path.join(dir_model_repo, \"torch_model_tbl.csv\"), header=None, names=HEAD_LIST) # load torch models list\n",
    "        mf = df[df.model_function == \"classifier\"]  # select all classifier models\n",
    "        for mp in mf.loc[:, ['model_path', 'preprocess_path']].values.tolist():\n",
    "            list_model_path.append([os.path.join(dir_model_repo, mp[0].split('/', 1)[-1]), \n",
    "                                    os.path.join(dir_model_repo, mp[1].split('/', 1)[-1])])\n",
    "        \n",
    "    for model_path in list_model_path:\n",
    "        torch_model_path = model_path[0]\n",
    "        pth_model_name = model_path[0].split('/')[-1].split('.')[0].split('_', 4)[-1]\n",
    "        path_trt_model = model_path[0].replace('.pth', '_trt.pth')\n",
    "        path_trt_model_preprocess = model_path[1]\n",
    "        print(\"path_trt_model: %s \\n\" % path_trt_model, \"path_trt_model_preprocess: %s\" % path_trt_model_preprocess)\n",
    "\n",
    "        if os.path.isfile(path_trt_model) and not re_convert:\n",
    "            pass\n",
    "        else:\n",
    "            model_type, model, preprocess_wrap = load_trained_model(torch_model_path, pth_model_name)\n",
    "            if preprocess_wrap is None:  # load pre-stored preprocess module of the trained model\n",
    "                preprocess = tv_classifier_preprocess()\n",
    "                preprocess.load_state_dict(torch.load(path_trt_model_preprocess))\n",
    "            else:  # used the preprocess from load_tune_pth_model\n",
    "                preprocess = preprocess_wrap[0]            \n",
    "            model_trt = trt_conversion(pth_model_name, model, preprocess)\n",
    "            save_trt_model(pth_model_name, model_type, model_trt, path_trt_model, path_trt_model_preprocess)\n",
    "            print(\"Building finished, and TRT model is saved in -- %s \\n\" % path_trt_model)\n",
    "    print(\"TRT model Conversion completed!\")    \n",
    "    button_OK.disabled=False\n",
    "    \n",
    "display(HBox([model_type_widget, model_path_widget]))\n",
    "\n",
    "button_OK = widgets.Button(description='OK', tooltip='Click to start', icon=\"start\")\n",
    "button_OK.style.button_color='lightblue'\n",
    "button_OK.on_click(start_trt_conversion)\n",
    "display(button_OK)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "Open `live_demo_light_trt.ipynb` to move JetBot with the TensorRT optimized model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
