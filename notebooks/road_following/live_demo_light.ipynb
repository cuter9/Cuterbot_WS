{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road Following - Live demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use model we trained to move jetBot smoothly on track. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. This script is only for simulating the models trained with `train_model_PC.ipynb`, the model train with \"train_model.ipynb\" should use live_demo.ipynb.\n",
    "2. Before executing this script, you should copy the whole directory (which should include `torch_model_tbl.csv` and folder `/road_following` ) and the trained model files saved inside it to `$Home/model_repo`, where HOME is the home directory of your Jetson Nano (here we assume `HOME=/home/cuterbot`). The trained model files are saved in the directory variable `DIR_RC_MODEL_REPO` you set in `train_model_PC.ipynb`, which should be in your PC used for training.\n",
    "3. If you are run the simulation with docker container, the `Home/model_repo` will be mapped to `/workspace/model_repo` which is the default of `MODEL_REPO_DIR_DOCKER` set in `model_selectio.py` located in directory `jetbot/utils/`. \n",
    "4. The pytorch model will be uploaded after you selected the model from the selection widget in the cell below.\n",
    "> Note: Please make sure the file has been uploaded fully from the log console of jupyter lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code below to initialize the PyTorch model. This should look very familiar from the training notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "from jetbot import RoadCruiser\n",
    "from jetbot.utils import model_selection\n",
    "import ipywidgets.widgets as widgets\n",
    "from ipywidgets.widgets import Box, HBox, VBox, Layout, Label, Output\n",
    "import traitlets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RC = RoadCruiser(init_sensor_rc=True)\n",
    "\n",
    "pth_ms = model_selection(core_library = \"Pytorch\")\n",
    "pth_ms.model_function = \"classifier\"\n",
    "\n",
    "model_type_widget = widgets.Select(options=pth_ms.model_type_list, value=pth_ms.model_type_list[0],\n",
    "                                      description='Model Type:')\n",
    "traitlets.dlink((pth_ms, 'model_type_list'), (model_type_widget, 'options'))\n",
    "traitlets.dlink((model_type_widget, 'value'), (pth_ms, 'model_type'))\n",
    "traitlets.dlink((pth_ms, 'model_type'), (RC, 'type_cruiser_model'))\n",
    "\n",
    "model_path_widget = widgets.Select(options=pth_ms.model_path_list, description='Model Path:',\n",
    "                                      layout=Layout(width='60%'))\n",
    "traitlets.dlink((pth_ms, 'model_path_list'), (model_path_widget, 'options'))\n",
    "traitlets.dlink((model_path_widget, 'value'), (pth_ms, 'model_path'))\n",
    "traitlets.dlink((pth_ms, 'model_path'), (RC, 'cruiser_model'))\n",
    "traitlets.dlink((pth_ms, 'preprocess_path'), (RC, 'cruiser_model_preprocess'))\n",
    "\n",
    "use_gpu_widget = widgets.RadioButtons(options=['gpu', 'cpu'], value='gpu', description='processor:', layout=Layout(width='15%'))\n",
    "traitlets.dlink((use_gpu_widget, 'value'), (RC, 'use_gpu'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load the trained weights from the ``best_steering_model_xy_model.pth`` file that you uploaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the model weights are located on the CPU memory execute the code below to transfer to the GPU device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating the Pre-Processing Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now loaded our model, but there's a slight issue. The format that we trained our model doesn't exactly match the format of the camera. To do that, we need to do some preprocessing. This involves the following steps:\n",
    "\n",
    "1. Convert from HWC layout to CHW layout\n",
    "2. Normalize using same parameters as we did during training (our camera provides values in [0, 255] range and training loaded images in [0, 1] range so we need to scale by 255.0\n",
    "3. Transfer the data from CPU memory to GPU memory\n",
    "4. Add a batch dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We've now defined our pre-processing function which can convert images from the camera format to the neural network input format.\n",
    "\n",
    "Now, let's start and display our camera. You should be pretty familiar with this by now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets\n",
    "from jetbot import bgr8_to_jpeg\n",
    "\n",
    "image_widget = ipywidgets.Image(width=300, height=300)\n",
    "# fps_widget = ipywidgets.FloatText(description='Capture rate')\n",
    "\n",
    "traitlets.dlink((RC.capturer, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "# traitlets.dlink((RC.camera, 'cap_time'), (fps_widget, 'value'))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will define sliders to control JetBot\n",
    "> Note: We have initialized the slider values for best known configurations, however, these might not work for your dataset, therefore, please increase or decrease the sliders according to your setup and environment\n",
    "\n",
    "1. Speed Control (speed_gain_slider): To start your JetBot increase ``speed_gain_slider`` \n",
    "2. Steering Gain Control (steering_gain_slider): If you see JetBot is wobbling, you need to reduce ``steering_gain_slider`` till it is smooth\n",
    "3. Steering Bias control (steering_bias_slider): If you see JetBot is biased towards extreme right or extreme left side of the track, you should control this slider till JetBot starts following line or track in the center.  This accounts for motor biases as well as camera offsets\n",
    "\n",
    "> Note: You should play around above-mentioned sliders with lower speed to get smooth JetBot road following behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_gain_slider = ipywidgets.FloatSlider(min=0, max=1, step=0.001, value=0.3, description='speed gain', readout_format='.3f')\n",
    "steering_gain_slider = ipywidgets.FloatSlider(min=0, max=0.5, step=0.001, value=0.22, description='steering gain', readout_format='.3f')\n",
    "steering_dgain_slider = ipywidgets.FloatSlider(min=0, max=2.0, step=0.001, value=1.15, description='steering kd', readout_format='.3f')\n",
    "steering_bias_slider = ipywidgets.FloatSlider(min=-0.1, max=0.1, step=0.001, value=-0.01, description='steering bias', readout_format='.3f')\n",
    "\n",
    "traitlets.dlink((speed_gain_slider, 'value'), (RC, 'speed_gain_rc'))\n",
    "traitlets.dlink((steering_gain_slider, 'value'), (RC, 'steering_gain_rc'))\n",
    "traitlets.dlink((steering_dgain_slider, 'value'), (RC, 'steering_dgain_rc'))\n",
    "traitlets.dlink((steering_bias_slider, 'value'), (RC, 'steering_bias_rc'))\n",
    "\n",
    "# VBox_image = ipywidgets.VBox([image_widget, fps_widget], layout=ipywidgets.Layout(align_self='center'))\n",
    "VBox_image = ipywidgets.VBox([image_widget], layout=ipywidgets.Layout(align_self='center'))\n",
    "VBox_control = ipywidgets.VBox([speed_gain_slider, steering_gain_slider, steering_dgain_slider, steering_bias_slider], layout=ipywidgets.Layout(align_self='center'))\n",
    "\n",
    "# display(ipywidgets.HBox([VBox_image, VBox_control]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's display some sliders that will let us see what JetBot is thinking.  The x and y sliders will display the predicted x, y values.\n",
    "\n",
    "The steering slider will display our estimated steering value.  Please remember, this value isn't the actual angle of the target, but simply a value that is\n",
    "nearly proportional.  When the actual angle is ``0``, this will be zero, and it will increase / decrease with the actual angle.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='x')\n",
    "y_slider = ipywidgets.FloatSlider(min=0, max=2.0, orientation='vertical', description='y')\n",
    "steering_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='steering')\n",
    "speed_slider = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='speed')\n",
    "\n",
    "traitlets.dlink((RC, 'x_slider'), (x_slider, 'value'))\n",
    "traitlets.dlink((RC, 'y_slider'), (y_slider, 'value'))\n",
    "traitlets.dlink((RC, 'steering_rc'), (steering_slider, 'value'))\n",
    "traitlets.dlink((RC, 'speed_rc'), (speed_slider, 'value'))\n",
    "\n",
    "Box_y_state = ipywidgets.HBox([y_slider, speed_slider])\n",
    "Box_x_state = ipywidgets.VBox([x_slider, steering_slider])\n",
    "Box_state = ipywidgets.VBox([Box_y_state, Box_x_state])\n",
    "\n",
    "# display(ipywidgets.HBox([y_slider, speed_slider]))\n",
    "# display(x_slider, steering_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a function that will get called whenever the camera's value changes. This function will do the following steps\n",
    "\n",
    "1. Pre-process the camera image\n",
    "2. Execute the neural network\n",
    "3. Compute the approximate steering value\n",
    "4. Control the motors using proportional / derivative control (PD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! We've created our neural network execution function, but now we need to attach it to the camera for processing.\n",
    "\n",
    "We accomplish that with the observe function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">WARNING: This code will move the robot!! Please make sure your robot has clearance and it is on Lego or Track you have collected data on. The road follower should work, but the neural network is only as good as the data it's trained on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! If your robot is plugged in it should now be generating new commands with each new camera frame. \n",
    "\n",
    "You can now place JetBot on  Lego or Track you have collected data on and see whether it can follow track.\n",
    "\n",
    "If you want to stop this behavior, you can unattach this callback by executing the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = Output(layout={'border': '2px solid black'})\n",
    "\n",
    "@out.capture()\n",
    "def start(change):\n",
    "    RC.start_rc(change)\n",
    "    \n",
    "@out.capture()\n",
    "def stop(change):\n",
    "    RC.stop_rc(change)\n",
    "    %reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(HBox([use_gpu_widget, model_type_widget, model_path_widget], layout={'border': '1px solid black'}))\n",
    "\n",
    "display(ipywidgets.HBox([Box_state, VBox_image, VBox_control]))\n",
    "\n",
    "button_start = ipywidgets.Button(description='Start', tooltip='Click to start running', icon='play')\n",
    "button_start.style.button_color='lightBlue'\n",
    "button_start.on_click(start)\n",
    "\n",
    "button_stop = ipywidgets.Button(description='Stop', tooltip='Click to stop running', icon=\"stop\")\n",
    "button_stop.style.button_color='Red'\n",
    "button_stop.on_click(stop)\n",
    "display(HBox([button_start, button_stop]))\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's close the camera conneciton properly so that we can use the camera in other notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conclusion\n",
    "That's it for this live demo! Hopefully you had some fun seeing your JetBot moving smoothly on track following the road!!!\n",
    "\n",
    "If your JetBot wasn't following road very well, try to spot where it fails. The beauty is that we can collect more data for these failure scenarios, and the JetBot should get even better :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
